{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b47acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "859f0c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "      <th>book_title</th>\n",
       "      <th>book_author</th>\n",
       "      <th>year_of_publication</th>\n",
       "      <th>publisher</th>\n",
       "      <th>img_m</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Category</th>\n",
       "      <th>num of ratings</th>\n",
       "      <th>num of ratings user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n/a, n/a, n/a</td>\n",
       "      <td>11676</td>\n",
       "      <td>34.7439</td>\n",
       "      <td>0399135782</td>\n",
       "      <td>9</td>\n",
       "      <td>The Kitchen God's Wife</td>\n",
       "      <td>Amy Tan</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>Putnam Pub Group</td>\n",
       "      <td>http://images.amazon.com/images/P/0399135782.0...</td>\n",
       "      <td>A Chinese immigrant who is convinced she is dy...</td>\n",
       "      <td>['Fiction']</td>\n",
       "      <td>116.0</td>\n",
       "      <td>2191.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knoxville, tennessee, usa</td>\n",
       "      <td>29526</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>0399135782</td>\n",
       "      <td>9</td>\n",
       "      <td>The Kitchen God's Wife</td>\n",
       "      <td>Amy Tan</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>Putnam Pub Group</td>\n",
       "      <td>http://images.amazon.com/images/P/0399135782.0...</td>\n",
       "      <td>A Chinese immigrant who is convinced she is dy...</td>\n",
       "      <td>['Fiction']</td>\n",
       "      <td>116.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>san antonio, texas, usa</td>\n",
       "      <td>46398</td>\n",
       "      <td>37.0000</td>\n",
       "      <td>0399135782</td>\n",
       "      <td>9</td>\n",
       "      <td>The Kitchen God's Wife</td>\n",
       "      <td>Amy Tan</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>Putnam Pub Group</td>\n",
       "      <td>http://images.amazon.com/images/P/0399135782.0...</td>\n",
       "      <td>A Chinese immigrant who is convinced she is dy...</td>\n",
       "      <td>['Fiction']</td>\n",
       "      <td>116.0</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>homer, alaska, usa</td>\n",
       "      <td>148712</td>\n",
       "      <td>34.7439</td>\n",
       "      <td>0399135782</td>\n",
       "      <td>10</td>\n",
       "      <td>The Kitchen God's Wife</td>\n",
       "      <td>Amy Tan</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>Putnam Pub Group</td>\n",
       "      <td>http://images.amazon.com/images/P/0399135782.0...</td>\n",
       "      <td>A Chinese immigrant who is convinced she is dy...</td>\n",
       "      <td>['Fiction']</td>\n",
       "      <td>116.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>colorado springs, colorado, usa</td>\n",
       "      <td>230522</td>\n",
       "      <td>52.0000</td>\n",
       "      <td>0399135782</td>\n",
       "      <td>7</td>\n",
       "      <td>The Kitchen God's Wife</td>\n",
       "      <td>Amy Tan</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>Putnam Pub Group</td>\n",
       "      <td>http://images.amazon.com/images/P/0399135782.0...</td>\n",
       "      <td>A Chinese immigrant who is convinced she is dy...</td>\n",
       "      <td>['Fiction']</td>\n",
       "      <td>116.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          location  user_id      age        isbn  rating  \\\n",
       "0                    n/a, n/a, n/a    11676  34.7439  0399135782       9   \n",
       "1        knoxville, tennessee, usa    29526  26.0000  0399135782       9   \n",
       "2          san antonio, texas, usa    46398  37.0000  0399135782       9   \n",
       "3               homer, alaska, usa   148712  34.7439  0399135782      10   \n",
       "4  colorado springs, colorado, usa   230522  52.0000  0399135782       7   \n",
       "\n",
       "               book_title book_author  year_of_publication         publisher  \\\n",
       "0  The Kitchen God's Wife     Amy Tan               1991.0  Putnam Pub Group   \n",
       "1  The Kitchen God's Wife     Amy Tan               1991.0  Putnam Pub Group   \n",
       "2  The Kitchen God's Wife     Amy Tan               1991.0  Putnam Pub Group   \n",
       "3  The Kitchen God's Wife     Amy Tan               1991.0  Putnam Pub Group   \n",
       "4  The Kitchen God's Wife     Amy Tan               1991.0  Putnam Pub Group   \n",
       "\n",
       "                                               img_m  \\\n",
       "0  http://images.amazon.com/images/P/0399135782.0...   \n",
       "1  http://images.amazon.com/images/P/0399135782.0...   \n",
       "2  http://images.amazon.com/images/P/0399135782.0...   \n",
       "3  http://images.amazon.com/images/P/0399135782.0...   \n",
       "4  http://images.amazon.com/images/P/0399135782.0...   \n",
       "\n",
       "                                             Summary     Category  \\\n",
       "0  A Chinese immigrant who is convinced she is dy...  ['Fiction']   \n",
       "1  A Chinese immigrant who is convinced she is dy...  ['Fiction']   \n",
       "2  A Chinese immigrant who is convinced she is dy...  ['Fiction']   \n",
       "3  A Chinese immigrant who is convinced she is dy...  ['Fiction']   \n",
       "4  A Chinese immigrant who is convinced she is dy...  ['Fiction']   \n",
       "\n",
       "   num of ratings  num of ratings user  \n",
       "0           116.0               2191.0  \n",
       "1           116.0                 62.0  \n",
       "2           116.0                132.0  \n",
       "3           116.0                 19.0  \n",
       "4           116.0                102.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('ratings_with_features.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15247d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the summary column for content based model\n",
    "summary_non_duplicates = df[['Summary']].drop_duplicates()\n",
    "summary_non_duplicates = pd.merge(df['book_title'].drop_duplicates(), summary_non_duplicates, left_index=True, right_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5945107b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 users\n",
      "5007 books\n"
     ]
    }
   ],
   "source": [
    "# Assign number of users and books to variables\n",
    "n_users = (df['user_id'].nunique())\n",
    "n_books = (df['book_title'].nunique())\n",
    "\n",
    "print(\"{} users\".format(n_users))\n",
    "print(\"{} books\".format(n_books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c2e765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to zip the user_id and book_id as some of the indexes are much bigger than the shape of our matrix\n",
    "users = dict(zip(df['user_id'].unique(), np.arange(0, n_users),))\n",
    "books = dict(zip(df['book_title'].unique(), np.arange(0, n_books)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c3f65f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 5007)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create user-book_rating matrix with users as rows and books as columns\n",
    "ratings = np.zeros((n_users, n_books))\n",
    "\n",
    "for row in df.itertuples():    \n",
    "    ratings[users[row[2]], books[row[6]]] = row[5]\n",
    "\n",
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41736768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 0.68%\n"
     ]
    }
   ],
   "source": [
    "# Lets check what is the sparsity of our matrix\n",
    "sparsity = float(len(ratings.nonzero()[0]))\n",
    "sparsity /= (ratings.shape[0] * ratings.shape[1])\n",
    "sparsity *= 100\n",
    "print('Sparsity: {:4.2f}%'.format(sparsity))\n",
    "del sparsity\n",
    "# The sparsity is low because the data set is huge\n",
    "# and the average number of ratings per user is relatively low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fab89a",
   "metadata": {},
   "source": [
    "# Collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "949957fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split data into train and test \n",
    "np.random.seed(42) \n",
    "\n",
    "train_data, test_data = train_test_split(ratings, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e9b062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity func\n",
    "def similarity(ratings, kind='user', epsilon=1e-9):\n",
    "    # epsilon -> small number for handling dived-by-zero errors\n",
    "    if kind == 'user':\n",
    "        sim = ratings.dot(ratings.T) + epsilon\n",
    "    elif kind == 'item':\n",
    "        sim = ratings.T.dot(ratings) + epsilon\n",
    "    norms = np.array([np.sqrt(np.diagonal(sim))])\n",
    "    return (sim / norms / norms.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50d6d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Lets find user and item similarity by using cosine similarity\n",
    "user_similarity = similarity(train_data, kind='user')\n",
    "item_similarity = similarity(train_data, kind='item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af063a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function\n",
    "def predict(ratings, similarity, type='user'):\n",
    "    if type =='user':\n",
    "        # Here we need to normalize the weight of each user's rating\n",
    "        # That is because each user may have different 'scale' of rating the book\n",
    "        # e.g. Someone really liked the book so gave it 4, another person gave 5 as he kind of liked it\n",
    "        \n",
    "        # That is why we will take mean of each user's ratings\n",
    "        mean_user_rating = ratings.mean(axis=1) # axis = 1 so it will take user's rating instead of mean rating of a book\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n",
    "        # dot function - matrix multiplications\n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    \n",
    "    # Here we do not need to normalize \n",
    "    elif type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)]) \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29bf01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_prediction = predict(train_data, item_similarity, type='item')\n",
    "user_prediction = predict(train_data, user_similarity, type='user')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e6a1ee",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11d93bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use RMSE as it seems to be most used in the industry for that kind of predictions\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Remember that we want to evaluate the non zero values - these are the actual ratings from user\n",
    "def get_rmse(preds, actual):\n",
    "    preds = preds[actual.nonzero()].flatten() # flatten function reduces the dimensionality of array to 1d\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "\n",
    "    return sqrt(mean_squared_error(preds, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8966aefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF RMSE: 8.010185676597473\n",
      "Item-based CF RMSE: 8.08024101974398\n"
     ]
    }
   ],
   "source": [
    "print('User-based CF RMSE: {}'.format(get_rmse(user_prediction, test_data)))\n",
    "print('Item-based CF RMSE: {}'.format(get_rmse(item_prediction, test_data)))\n",
    "\n",
    "# Clean the kernel from this data as we will not need this anymore\n",
    "del user_prediction\n",
    "del item_prediction\n",
    "del user_similarity\n",
    "del item_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30f7146c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF RMSE (All data): 7.258342718896441\n",
      "Item-based CF RMSE (All data): 7.33069003330777\n"
     ]
    }
   ],
   "source": [
    "# Let's now retrain the data on the whole dataset\n",
    "user_similarity_all = similarity(ratings,)\n",
    "item_similarity_all = similarity(ratings.T)\n",
    "\n",
    "item_prediction_all = predict(ratings, item_similarity_all, type='item')\n",
    "user_prediction_all = predict(ratings, user_similarity_all, type='user')\n",
    "\n",
    "\n",
    "print('User-based CF RMSE (All data): {}'.format(get_rmse(user_prediction_all, ratings)))\n",
    "print('Item-based CF RMSE (All data): {}'.format(get_rmse(item_prediction_all, ratings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7589cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Content-Based models based on the summary of each movie in the daaset\n",
    "# We will use TFIDF technique with ngrams 1 and 2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# We are using this technique instead of countVectorizer because:\n",
    "# TFIDF will give importance to the rare words, that we will need in finding similarities in the book's summary\n",
    "\n",
    "tf_transformer = TfidfVectorizer(ngram_range=(1, 2)).fit(summary_non_duplicates['Summary'])\n",
    "X_train_tf = tf_transformer.transform(summary_non_duplicates['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "868d798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "summary_similarity = pd.DataFrame(cosine_similarity(X_train_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "049c2fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pearson correlation RMSE: 8.06885197254186\n"
     ]
    }
   ],
   "source": [
    "# The pearson correlation does not depend on the scale of data\n",
    "# It means that very popular books will not be favored\n",
    "item_correlation = 1 - pairwise_distances(ratings.T, metric='correlation')\n",
    "item_correlation[np.isnan(item_correlation)] = 0\n",
    "\n",
    "print('The pearson correlation RMSE: {}'.format(get_rmse(item_correlation, ratings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8677a499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Into the Wild': 0.07713094551124931,\n",
       " 'She Went All the Way (Avon Light Contemporary Romances)': 0.07426472063308022,\n",
       " 'No Sanctuary': 0.07407931519820433,\n",
       " 'Bellwether': 0.07221418881836919,\n",
       " 'The English Patient': 0.0601836994314694}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SUMMARY SIMILARITY\n",
    "# example\n",
    "bookname = 'The Testament'\n",
    "\n",
    "# map keys\n",
    "bookskey = list(books)\n",
    "userkey = list(users)\n",
    "\n",
    "# store the book and its similarity\n",
    "book_similarity_by_summary = {}\n",
    "\n",
    "# Get the index of the book in the summary similarity metrix\n",
    "target_index_summary = summary_non_duplicates[summary_non_duplicates['book_title'] == bookname].index[0]\n",
    "\n",
    "# Get the top 5 similar books to given book\n",
    "top5_books_id_summary = summary_similarity.loc[target_index_summary].sort_values(ascending=False)[1:6]\n",
    "\n",
    "# Map it to the dictionary\n",
    "for index in top5_books_id_summary.index:\n",
    "    book_similarity_by_summary[summary_non_duplicates.iloc[index]['book_title']] = top5_books_id_summary[index]\n",
    "\n",
    "book_similarity_by_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6125e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the top ids with largest similarity to given book \n",
    "def top_k_movies(similarity, book_idx, k=10):\n",
    "    return [x for x in np.argsort(similarity[book_idx,:])[:-k-1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7274205c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The Runaway Jury': 0.28458417625889515,\n",
       " 'The Street Lawyer': 0.27400003713158927,\n",
       " 'The Chamber': 0.2556271962622344,\n",
       " 'Critical Mass': 0.24025795103115202,\n",
       " 'The Brethren': 0.23786493340704284}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Item SIMILARITY\n",
    "book_id = books[bookname]\n",
    "\n",
    "# map keys\n",
    "bookskey = list(books)\n",
    "userkey = list(users)\n",
    "\n",
    "book_similarity_by_item = {}\n",
    "\n",
    "top5_books_id_item = top_k_movies(item_similarity_all, book_id, 6)\n",
    "\n",
    "# map the values of the most similar books with its title\n",
    "for id_ in top5_books_id_item[1:]:\n",
    "    book_similarity_by_item[bookskey[id_]] = item_similarity_all[id_, book_id]\n",
    "\n",
    "book_similarity_by_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ffabf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The Five People You Meet in Heaven': 0.2542364042251464,\n",
       " 'The Testament': 0.2432376175927512,\n",
       " 'All That Remains (Kay Scarpetta Mysteries (Paperback))': 0.22982788912366536,\n",
       " 'To Kill a Mockingbird': 0.22189866861575036,\n",
       " \"Our Dumb Century: The Onion Presents 100 Years of Headlines from America's Finest News Source\": 0.2063009070382933}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlation\n",
    "book_similarity_by_corr = {}\n",
    "\n",
    "top5_books_id_corr = top_k_movies(item_correlation, book_id, 6)\n",
    "\n",
    "for id_ in top5_books_id_corr[1:]:\n",
    "#     print(df.iloc[id_]['book_title'])\n",
    "    book_similarity_by_corr[df.iloc[id_]['book_title']] = item_correlation[book_id, id_]\n",
    "\n",
    "book_similarity_by_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3604f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
